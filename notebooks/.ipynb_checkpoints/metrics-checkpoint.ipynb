{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85ef16ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d7d451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGINGFACE_API_KEY = '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'\n",
    "\n",
    "HUGGINGFACE_TASKS = {\n",
    "    'semantic_similarity': 'https://qv0ojt0iijtajfql.us-east-1.aws.endpoints.huggingface.cloud'\n",
    "}\n",
    "\n",
    "\n",
    "def protected_hf_endpoint(api_url, payload):\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {HUGGINGFACE_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(api_url, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def run_hf_task(task_type, payload):\n",
    "    api_url = HUGGINGFACE_TASKS.get(task_type)\n",
    "    if not api_url:\n",
    "        raise ValueError(f\"Invalid task type: {task_type}\")\n",
    "    return protected_hf_endpoint(api_url, payload)\n",
    "\n",
    "\n",
    "def semantic_similarity(source_text, texts):\n",
    "    if type(texts) == str:\n",
    "        texts = [texts]\n",
    "    payload = {\n",
    "        \"inputs\": {\n",
    "            \"source_sentence\": source_text,\n",
    "            \"sentences\": texts\n",
    "        },\n",
    "    }\n",
    "    response = run_hf_task('semantic_similarity', payload).get('similarities')\n",
    "    if response:\n",
    "        return response if len(response) > 1 else response[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c987d929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8614885807037354\n",
      "[0.8614886403083801, 0.5496235489845276]\n"
     ]
    }
   ],
   "source": [
    "print(semantic_similarity('I am a happy person', ['I am a happy human']))\n",
    "print(semantic_similarity('I am a happy person', ['I am a happy human', 'That is a happy dog']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "57fe7167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "\n",
    "def calculate_bleu(reference, candidate):  \n",
    "    # Tokenize the sentences\n",
    "    reference = reference.split()\n",
    "    candidate = candidate.split()\n",
    "    # BLEU expects a list of reference sentences\n",
    "    return sentence_bleu([reference], candidate)\n",
    "\n",
    "\n",
    "def exact_match(text1, text2):\n",
    "    return text1 == text2\n",
    "\n",
    "def rouge_n(reference, generated, n=1):\n",
    "    # Placeholder function for ROUGE-N\n",
    "    # In practice, consider using a library like 'rouge' for comprehensive calculations\n",
    "    # This is a very basic and naive implementation\n",
    "    ref_ngrams = set([reference[i:i+n] for i in range(len(reference)-n+1)])\n",
    "    gen_ngrams = set([generated[i:i+n] for i in range(len(generated)-n+1)])\n",
    "    overlap = len(ref_ngrams.intersection(gen_ngrams))\n",
    "    return overlap / len(ref_ngrams)\n",
    "\n",
    "\n",
    "def word_error_rate(reference, generated):\n",
    "    # A basic implementation of WER\n",
    "    ref_words = reference.split()\n",
    "    gen_words = generated.split()\n",
    "    edits = edit_distance(ref_words, gen_words)\n",
    "    return edits / len(ref_words)\n",
    "\n",
    "def edit_distance(s1, s2):\n",
    "    # Compute the edit distance between two lists\n",
    "    if len(s1) == 0:\n",
    "        return len(s2)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "    \n",
    "    if s1[-1] == s2[-1]:\n",
    "        cost = 0\n",
    "    else:\n",
    "        cost = 1\n",
    "\n",
    "    return min([edit_distance(s1[:-1], s2) + 1,\n",
    "                edit_distance(s2[:-1], s1) + 1,\n",
    "                edit_distance(s1[:-1], s2[:-1]) + cost])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "20161287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.821831989445342e-231"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that bleu is not a great metric for short sequences. This is just a demonstration\n",
    "    \n",
    "calculate_bleu('hi', 'hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d6b12282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_text_similarity(test_cases):\n",
    "    \n",
    "    for text1, text2, expected_exact, expected_similarity, expected_rouge, expected_wer, expected_bleu in test_cases:\n",
    "        # Check exact match\n",
    "        assert exact_match(text1, text2) == expected_exact, f\"Failed for exact match test: {text1} vs {text2}\"\n",
    "\n",
    "        # Check semantic similarity\n",
    "        similarity_score = semantic_similarity(text1, text2)\n",
    "        assert abs(similarity_score - expected_similarity) <= 0.1, f\"Failed for semantic similarity test: {text1} vs {text2}. Expected {expected_similarity}, got {similarity_score}\"\n",
    "\n",
    "        # Check ROUGE-N\n",
    "        rouge_score = rouge_n(text1, text2, 1)\n",
    "        assert abs(rouge_score - expected_rouge) <= 0.1, f\"Failed for ROUGE-N test: {text1} vs {text2}. Expected {expected_rouge}, got {rouge_score}\"\n",
    "\n",
    "        # Check WER\n",
    "        wer = word_error_rate(text1, text2)\n",
    "        assert abs(wer - expected_wer) <= 0.1, f\"Failed for WER test: {text1} vs {text2}. Expected {expected_wer}, got {wer}\"\n",
    "        \n",
    "        # Check BLEU score\n",
    "        bleu_score = calculate_bleu(text1, text2)\n",
    "        assert abs(bleu_score - expected_bleu) <= 0.1, f\"Failed for BLEU test: {text1} vs {text2}. Expected {expected_bleu}, got {bleu_score}\"\n",
    "\n",
    "\n",
    "    print(\"All tests passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271c0360",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_similarity(\n",
    "    test_cases = [\n",
    "            (\"I love dogs\", \"I love dogs\",    True,  1,   0.9, 0,  0),\n",
    "            (\"I love dogs\", \"I love canines\", False, 0.9, 0.7, .34, 0)\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885bca5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23df4a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_similarity(\n",
    "    test_cases = [\n",
    "            (\"I love dogs\", \"I love dogs\",    True,  1,   0.9, 0,  0),\n",
    "            (\"I love dogs\", \"I love canines\", False, 0.9, 0.7, .34, 0), \n",
    "            (\"I love dogs\", \"I like coffee\",  False, 0.5, 0.4, .5, 0)\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de68708c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa66619",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e32924",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
