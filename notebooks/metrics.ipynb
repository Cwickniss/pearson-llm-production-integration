{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85ef16ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "HUGGINGFACE_API_KEY = 'hf_****'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d7d451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGINGFACE_TASKS = {\n",
    "    'semantic_similarity': 'https://qv0ojt0iijtajfql.us-east-1.aws.endpoints.huggingface.cloud'\n",
    "}\n",
    "\n",
    "\n",
    "def protected_hf_endpoint(api_url, payload):\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {HUGGINGFACE_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(api_url, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "\n",
    "def run_hf_task(task_type, payload):\n",
    "    api_url = HUGGINGFACE_TASKS.get(task_type)\n",
    "    if not api_url:\n",
    "        raise ValueError(f\"Invalid task type: {task_type}\")\n",
    "    return protected_hf_endpoint(api_url, payload)\n",
    "\n",
    "\n",
    "def semantic_similarity(source_text, texts):\n",
    "    if type(texts) == str:\n",
    "        texts = [texts]\n",
    "    payload = {\n",
    "        \"inputs\": {\n",
    "            \"source_sentence\": source_text,\n",
    "            \"sentences\": texts\n",
    "        },\n",
    "    }\n",
    "    response = run_hf_task('semantic_similarity', payload).get('similarities')\n",
    "    if response:\n",
    "        return response if len(response) > 1 else response[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c987d929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8614885807037354\n",
      "[0.8614886403083801, 0.5496235489845276]\n"
     ]
    }
   ],
   "source": [
    "print(semantic_similarity('I am a happy person', ['I am a happy human']))\n",
    "print(semantic_similarity('I am a happy person', ['I am a happy human', 'That is a happy dog']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "57fe7167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "\n",
    "def calculate_bleu(reference, candidate):  \n",
    "    # Tokenize the sentences\n",
    "    reference = reference.split()\n",
    "    candidate = candidate.split()\n",
    "    # BLEU expects a list of reference sentences\n",
    "    return sentence_bleu([reference], candidate)\n",
    "\n",
    "\n",
    "def exact_match(text1, text2):\n",
    "    return text1 == text2\n",
    "\n",
    "def rouge_n(reference, generated, n=1):\n",
    "    # Placeholder function for ROUGE-N\n",
    "    # In practice, consider using a library like 'rouge' for comprehensive calculations\n",
    "    # This is a very basic and naive implementation\n",
    "    ref_ngrams = set([reference[i:i+n] for i in range(len(reference)-n+1)])\n",
    "    gen_ngrams = set([generated[i:i+n] for i in range(len(generated)-n+1)])\n",
    "    overlap = len(ref_ngrams.intersection(gen_ngrams))\n",
    "    return overlap / len(ref_ngrams)\n",
    "\n",
    "\n",
    "def word_error_rate(reference, generated):\n",
    "    # A basic implementation of WER\n",
    "    ref_words = reference.split()\n",
    "    gen_words = generated.split()\n",
    "    edits = edit_distance(ref_words, gen_words)\n",
    "    return edits / len(ref_words)\n",
    "\n",
    "def edit_distance(s1, s2):\n",
    "    # Compute the edit distance between two lists\n",
    "    if len(s1) == 0:\n",
    "        return len(s2)\n",
    "    if len(s2) == 0:\n",
    "        return len(s1)\n",
    "    \n",
    "    if s1[-1] == s2[-1]:\n",
    "        cost = 0\n",
    "    else:\n",
    "        cost = 1\n",
    "\n",
    "    return min([edit_distance(s1[:-1], s2) + 1,\n",
    "                edit_distance(s2[:-1], s1) + 1,\n",
    "                edit_distance(s1[:-1], s2[:-1]) + cost])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "60d6132a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.821831989445342e-231"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that bleu is not a great metric for short sequences. This is just a demonstration\n",
    "    \n",
    "calculate_bleu('hi', 'hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "88c63c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_text_similarity(test_cases):\n",
    "    \n",
    "    for text1, text2, expected_exact, expected_similarity, expected_rouge, expected_wer, expected_bleu in test_cases:\n",
    "        # Check exact match\n",
    "        assert exact_match(text1, text2) == expected_exact, f\"Failed for exact match test: {text1} vs {text2}\"\n",
    "\n",
    "        # Check semantic similarity\n",
    "        similarity_score = semantic_similarity(text1, text2)\n",
    "        assert abs(similarity_score - expected_similarity) <= 0.1, f\"Failed for semantic similarity test: {text1} vs {text2}. Expected {expected_similarity}, got {similarity_score}\"\n",
    "\n",
    "        # Check ROUGE-N\n",
    "        rouge_score = rouge_n(text1, text2, 1)\n",
    "        assert abs(rouge_score - expected_rouge) <= 0.1, f\"Failed for ROUGE-N test: {text1} vs {text2}. Expected {expected_rouge}, got {rouge_score}\"\n",
    "\n",
    "        # Check WER\n",
    "        wer = word_error_rate(text1, text2)\n",
    "        assert abs(wer - expected_wer) <= 0.1, f\"Failed for WER test: {text1} vs {text2}. Expected {expected_wer}, got {wer}\"\n",
    "        \n",
    "        # Check BLEU score\n",
    "        bleu_score = calculate_bleu(text1, text2)\n",
    "        assert abs(bleu_score - expected_bleu) <= 0.1, f\"Failed for BLEU test: {text1} vs {text2}. Expected {expected_bleu}, got {bleu_score}\"\n",
    "\n",
    "\n",
    "    print(\"All tests passed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "061f5a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "test_text_similarity(\n",
    "    test_cases = [\n",
    "            (\"I love dogs\", \"I love dogs\",    True,  1,   0.9, 0,  0),\n",
    "            (\"I love dogs\", \"I love canines\", False, 0.9, 0.7, .34, 0)\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22de8a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "23df4a81",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Failed for ROUGE-N test: I love dogs vs I like coffee. Expected 0.4, got 0.5555555555555556",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtest_text_similarity\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_cases\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mI love dogs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mI love dogs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m    \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mI love dogs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mI love canines\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.34\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mI love dogs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mI like coffee\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [63], line 13\u001b[0m, in \u001b[0;36mtest_text_similarity\u001b[0;34m(test_cases)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Check ROUGE-N\u001b[39;00m\n\u001b[1;32m     12\u001b[0m rouge_score \u001b[38;5;241m=\u001b[39m rouge_n(text1, text2, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(rouge_score \u001b[38;5;241m-\u001b[39m expected_rouge) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed for ROUGE-N test: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m vs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext2\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_rouge\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrouge_score\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Check WER\u001b[39;00m\n\u001b[1;32m     16\u001b[0m wer \u001b[38;5;241m=\u001b[39m word_error_rate(text1, text2)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Failed for ROUGE-N test: I love dogs vs I like coffee. Expected 0.4, got 0.5555555555555556"
     ]
    }
   ],
   "source": [
    "test_text_similarity(\n",
    "    test_cases = [\n",
    "            (\"I love dogs\", \"I love dogs\",    True,  1,   0.9, 0,  0),\n",
    "            (\"I love dogs\", \"I love canines\", False, 0.9, 0.7, .34, 0), \n",
    "            (\"I love dogs\", \"I like coffee\",  False, 0.5, 0.4, .5, 0)\n",
    "        ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea2a825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8937c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1811e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
